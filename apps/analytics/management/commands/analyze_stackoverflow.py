import csv
import re
import heapq
from collections import defaultdict
from pathlib import Path
from xml.etree.ElementTree import iterparse
from datetime import datetime, timedelta, timezone

from django.db.models import F
from django.core.management.base import BaseCommand
from django.db import transaction

from apps.trends.models import TechStack, Article, ArticleStack


# ÌÜ†ÌÅ∞ Ï∂îÏ∂úÏö©
TOKEN_RE = re.compile(r"[a-z0-9\+\#\.\-]+")

# ÎÖ∏Ïù¥Ï¶àÎ°ú ÌäÄÎäî Í∏∞Ïà†Î™Ö ÌïÑÌÑ∞ÎßÅ
NOISE_TECHS = {
    "d", "q",
    "make", "simple", "mean", "parse", "render", "echo", "stream", "buffer", "heap",
    "box", "hub", "dash", "flux", "salt", "ent", "tower", "buddy",
    "play", "linear", "segment", "prism", "foundation", "slick", "realm", "crystal",
}

# ÌïÑÌÑ∞ÎßÅ ÌïòÎ©¥ ÏïàÎêòÎäî Í∏∞Ïà†Î™Ö
KNOWN_SHORT_TECHS = {"go", "r", "d3", "qt"}


def is_noise_tech(normalized_tech: str) -> bool:
    if normalized_tech in KNOWN_SHORT_TECHS:
        return False
    if not normalized_tech:
        return True
    if normalized_tech in NOISE_TECHS:
        return True

    toks = TOKEN_RE.findall(normalized_tech)
    if not toks:
        return True

    # Îã®Ïùº ÌÜ†ÌÅ∞Ïù∏Îç∞ "ÏïåÌååÎ≤≥Îßå" && "Í∏∏Ïù¥ <= 2" Ïù¥Î©¥ ÎÖ∏Ïù¥Ï¶àÎ°ú Ï≤òÎ¶¨
    if len(toks) == 1:
        t = toks[0]
        if t.isalpha() and len(t) <= 2:
            return True
    return False


# Í∏∞Ïà†Î™Ö ÌëúÏ§ÄÌôî (Í≥µÎ∞± Ï†ïÎ¶¨, ÏÜåÎ¨∏ÏûêÌôî)
def normalize_spaces(s: str) -> str:
    return " ".join((s or "").split())


def normalize_tech_name(name: str) -> str:
    return normalize_spaces(name).lower()


# xml ÌÉúÍ∑∏ ÌëúÏ§ÄÌôî
def normalize_tags(tags: str) -> str:
    t = tags or ""
    if "|" in t:
        # |tag|tag| ÌòïÌÉú
        return normalize_spaces(t.replace("|", " "))
    # <tag><tag> ÌòïÌÉú
    return normalize_spaces(t.replace("><", "> <").replace("<", " ").replace(">", " "))


# xml Í≤åÏãúÍ∏Ä ÌÖçÏä§Ìä∏ ÌëúÏ§ÄÌôî
def normalize_post_text(title: str, body: str, tags: str) -> str:
    tags_clean = normalize_tags(tags)
    text = f"{title or ''} {body or ''} {tags_clean}"
    return normalize_spaces(text).lower()


# tech_stacks_source.csv Î°úÎìú
def load_techs_from_csv(csv_path: Path) -> list[str]:
    """Name Ïª¨ÎüºÎßå ÏùΩÏñ¥ÏÑú Í∏∞Ïà† Î™©Î°ù ÏÉùÏÑ±"""
    techs: list[str] = []
    with csv_path.open("r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        fields = reader.fieldnames or []
        col = "Name" if "Name" in fields else ("name" if "name" in fields else None)
        if not col:
            raise ValueError(f"CSV must contain a 'Name' column. Found: {fields}")

        for row in reader:
            tech = normalize_tech_name(row.get(col) or "")
            if tech:
                techs.append(tech)

    # Ï§ëÎ≥µ Ï†úÍ±∞ (ÏàúÏÑú Ïú†ÏßÄ)
    seen = set()
    uniq = []
    for t in techs:
        if t not in seen:
            uniq.append(t)
            seen.add(t)
    return uniq


# CreationDate -> datetime(UTC)
def parse_creation_dt(s: str) -> datetime | None:
    if not s:
        return None
    try:
        dt = datetime.fromisoformat(s)
        if dt.tzinfo is None:
            return dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except ValueError:
        return None


# XML Ïä§Ìä∏Î¶¨Î∞ç ÌååÏã±
def iter_posts(posts_xml_path: Path):
    context = iterparse(posts_xml_path, events=("end",))
    try:
        for _, elem in context:
            if elem.tag != "row":
                continue

            a = elem.attrib
            post_id = a.get("Id") or ""
            post_type = a.get("PostTypeId") or ""  # 1=Question, 2=Answer
            title = a.get("Title") or ""
            body = a.get("Body") or ""
            tags = a.get("Tags") or ""
            view_count_raw = a.get("ViewCount") or "0"
            created_raw = a.get("CreationDate") or ""

            try:
                view_count = int(view_count_raw)
            except ValueError:
                view_count = 0

            created_at = parse_creation_dt(created_raw)

            # Î©îÎ™®Î¶¨ Î∞©ÏßÄ
            elem.clear()

            yield post_id, post_type, title, body, tags, view_count, created_at
    except Exception as e:
        import sys
        print(f"Warning: XML parsing error occurred (file may be incomplete): {e}", file=sys.stderr)
        print("Processed data up to the error point will be used.", file=sys.stderr)


# "react native" Í∞ôÏù¥ Ïó¨Îü¨ Îã®Ïñ¥ Í∏∞Ïà†Ïù¥ Ïó∞ÏÜçÏúºÎ°ú Îì±Ïû•ÌïòÎäîÏßÄ ÌôïÏù∏
def tokens_match(tech_tokens: list[str], post_tokens: list[str]) -> bool:
    if not tech_tokens:
        return False
    L = len(tech_tokens)
    if L > len(post_tokens):
        return False
    for i in range(len(post_tokens) - L + 1):
        if post_tokens[i:i + L] == tech_tokens:
            return True
    return False


# ÌõÑÎ≥¥ Ï∂ïÏÜå Ïù∏Îç±Ïä§
def build_tech_index(techs: list[str]):
    single_index = defaultdict(list)  # token -> [tech]
    multi_index = defaultdict(list)   # first_token -> [tech]
    tech_tokens_map: dict[str, list[str]] = {}

    for tech in techs:
        tokens = TOKEN_RE.findall(tech)
        if not tokens:
            continue
        tech_tokens_map[tech] = tokens

        if len(tokens) == 1:
            single_index[tokens[0]].append(tech)
        else:
            multi_index[tokens[0]].append(tech)

    return single_index, multi_index, tech_tokens_map


def find_max_creation_dt(posts_xml_path: Path) -> datetime | None:
    """XML ÏïàÏóêÏÑú Í∞ÄÏû• ÏµúÏã† CreationDate Ï∞æÍ∏∞ (QuestionÎßå)"""
    max_dt = None
    for _, post_type, _, _, _, _, created_at in iter_posts(posts_xml_path):
        if post_type != "1":
            continue
        if created_at is None:
            continue
        if max_dt is None or created_at > max_dt:
            max_dt = created_at
    return max_dt


def parse_anchor_dt(s: str) -> datetime | None:
    return parse_creation_dt(s) if s else None


WINDOW_TO_DELTA = {
    "7d": timedelta(days=7),
    "1m": timedelta(days=30),
    "3m": timedelta(days=90),
}


class Command(BaseCommand):
    help = (
        "Analyze StackOverflow Posts.xml by simple string matching against our tech stack CSV (Name column). "
        "Counts mentions and aggregates view counts, then sorts by total views."
    )

    def add_arguments(self, parser):
        parser.add_argument("--posts", required=True, help="Path to Posts.xml (or sample xml)")
        parser.add_argument("--stacks", required=True, help="Path to tech_stacks_source.csv (Name/Image/Link)")
        parser.add_argument("--out", required=True, help="Output CSV path")
        parser.add_argument("--limit", type=int, default=0, help="Optional: limit number of rows to scan (0=no limit)")
        parser.add_argument("--progress", type=int, default=10000, help="Print progress every N rows")

        parser.add_argument(
            "--with-top-posts",
            action="store_true",
            help="If set, store tech-wise top posts into out.csv as top_posts column.",
        )

        parser.add_argument(
            "--detail-tech",
            default="",
            help="Extract top posts for this tech (normalized, e.g. 'git'). Leave empty to skip.",
        )
        parser.add_argument(
            "--topn",
            type=int,
            default=10,
            help="How many top posts to keep for --detail-tech (default 10)",
        )
        parser.add_argument(
            "--detail-out",
            default="",
            help="Optional: output CSV path for --detail-tech results. "
                 "If empty, auto-generate next to --out (e.g. git_top_posts_10.csv).",
        )

        parser.add_argument(
            "--save-db",
            action="store_true",
            help="If set, save Article and ArticleStack into DB.",
        )

        parser.add_argument(
            "--window",
            choices=["", "7d", "1m", "3m"],
            default="",
            help="Filter posts by CreationDate: 7d=last 7 days, 1m=last 30 days, 3m=last 90 days. Empty=all time.",
        )

        parser.add_argument(
            "--posts-out",
            default="",
            help="Optional: output CSV path for filtered posts list (post_id,created_at,url,title,view_count,tags).",
        )
        parser.add_argument(
            "--posts-order",
            choices=["", "views", "date"],
            default="",
            help="Sort order for --posts-out CSV. views=by view_count desc, date=by created_at desc. Empty=keep scan order.",
        )

        parser.add_argument(
            "--window-base",
            choices=["now", "max"],
            default="max",
            help="Base time for --window. now=use current time, max=use max CreationDate inside XML (recommended for old dumps).",
        )
        parser.add_argument(
            "--anchor",
            default="",
            help="Optional: anchor datetime (ISO8601). If set, use this as base time for --window. Example: 2012-01-01T00:00:00+00:00",
        )

    def handle(self, *args, **options):
        posts_path = Path(options["posts"]).expanduser()
        stacks_path = Path(options["stacks"]).expanduser()
        out_path = Path(options["out"]).expanduser()

        limit = int(options["limit"])
        progress = int(options["progress"])

        with_top_posts = bool(options["with_top_posts"])
        topn = int(options["topn"])

        detail_tech = normalize_tech_name(options.get("detail_tech") or "")
        detail_out_opt = (options.get("detail_out") or "").strip()

        save_db = bool(options.get("save_db"))

        window = (options.get("window") or "").strip()
        posts_out_opt = (options.get("posts_out") or "").strip()
        posts_order = (options.get("posts_order") or "").strip()

        window_base = (options.get("window_base") or "max").strip()
        anchor_raw = (options.get("anchor") or "").strip()

        if not posts_path.exists():
            self.stderr.write(self.style.ERROR(f"Posts file not found: {posts_path}"))
            return
        if not stacks_path.exists():
            self.stderr.write(self.style.ERROR(f"Stacks CSV not found: {stacks_path}"))
            return

        # ---- window cutoff Í≥ÑÏÇ∞ (Îã®Ïùº window) ----
        cutoff: datetime | None = None

        # üê∂ [ADD] posts-out windows(7d/1m/3m) Í≥ÑÏÇ∞Ïö© base_dt/cutoffs Ï§ÄÎπÑ
        # - anchor > window-base(now/max) ÏàúÏÑúÎ°ú base_dt Í≤∞Ï†ï
        anchor_dt_for_windows = parse_anchor_dt(anchor_raw) if anchor_raw else None
        if anchor_dt_for_windows is not None:
            base_dt_for_windows = anchor_dt_for_windows
        else:
            if window_base == "now":
                base_dt_for_windows = datetime.now(timezone.utc)
            else:
                base_dt_for_windows = find_max_creation_dt(posts_path)
                if base_dt_for_windows is None:
                    self.stderr.write(self.style.ERROR("Could not determine max CreationDate from XML."))
                    return

        cutoffs_for_windows = {
            "7d": base_dt_for_windows - WINDOW_TO_DELTA["7d"],
            "1m": base_dt_for_windows - WINDOW_TO_DELTA["1m"],
            "3m": base_dt_for_windows - WINDOW_TO_DELTA["3m"],
        }
        # üê∂ [ADD] Î°úÍ∑∏ (posts-out/windows Í∏∞Ï§ÄÏãúÍ∞Å)
        self.stdout.write(
            f"[windows] base_dt={base_dt_for_windows.isoformat()} cutoffs="
            f"{ {k: v.isoformat() for k, v in cutoffs_for_windows.items()} }"
        )

        if window:
            # üê∂ [MOD] Îã®Ïùº window cutoffÎèÑ ÎèôÏùºÌïú base_dt Î°ú Í≥ÑÏÇ∞(ÏùºÍ¥ÄÏÑ±)
            cutoff = base_dt_for_windows - WINDOW_TO_DELTA[window]
            self.stdout.write(f"[window] base_dt={base_dt_for_windows.isoformat()} cutoff={cutoff.isoformat()} window={window}")

        # ---- Í∏∞Ïà† Î™©Î°ù Î°úÎìú ----
        try:
            techs = load_techs_from_csv(stacks_path)
        except Exception as e:
            self.stderr.write(self.style.ERROR(f"Failed to load stacks CSV: {e}"))
            return

        techs = [t for t in techs if not is_noise_tech(t)]
        if not techs:
            self.stderr.write(self.style.ERROR("No techs loaded from CSV (Name column empty?)."))
            return

        # ---- DB TechStack Îß§Ìïë (save_dbÏùº ÎïåÎßå) ----
        db_tech_map: dict[str, TechStack] = {}
        if save_db:
            qs = TechStack.objects.filter(is_deleted=False).only("id", "name")
            db_tech_map = {normalize_tech_name(t.name): t for t in qs}
            if not db_tech_map:
                self.stderr.write(self.style.ERROR("TechStack table is empty. Seed TechStack first."))
                return

            # CSV Í∏∞Ïà† Ï§ë DBÏóê Ï°¥Ïû¨ÌïòÎäî Í≤ÉÎßå Ïú†ÏßÄ
            techs = [t for t in techs if t in db_tech_map]
            if not techs:
                self.stderr.write(self.style.ERROR("No CSV techs matched TechStack(name) in DB after normalization."))
                return

        tech_set = set(techs)
        if detail_tech and detail_tech not in tech_set:
            self.stderr.write(self.style.ERROR(f"--detail-tech '{detail_tech}' not found in stacks CSV"))
            return

        single_index, multi_index, tech_tokens_map = build_tech_index(techs)

        # ---- ÏßëÍ≥Ñ Íµ¨Ï°∞ ----
        mention_count = defaultdict(int)      # tech -> Ïñ∏Í∏âÎêú Í≤åÏãúÍ∏Ä Ïàò
        total_views = defaultdict(int)        # tech -> Ï°∞ÌöåÏàò ÎàÑÏ†Å Ìï©
        top_posts_by_tech = defaultdict(list) # tech -> heap(view_count, post_id, title)
        detail_heap = []                      # heap(view_count, post_id, title)
        filtered_posts_rows = []              # posts-out rows

        scanned = 0

        for post_id, post_type, title, body, tags, view_count, created_at in iter_posts(posts_path):
            if post_type != "1":
                continue

            # window ÌïÑÌÑ∞ (Îã®Ïùº window ÏòµÏÖò Ïú†ÏßÄ)
            if cutoff is not None:
                if created_at is None or created_at < cutoff:
                    continue

            scanned += 1
            if limit and scanned > limit:
                break

            # üê∂ [ADD] posts-outÏö© windows Ïª¨Îüº Í≥ÑÏÇ∞ (7d/1m/3m ÏÜåÏÜç)
            windows_bucket = ""
            if created_at is not None:
                buckets = []
                # 7d ‚äÇ 1m ‚äÇ 3m Íµ¨Ï°∞Îùº created_atÏù¥ ÏµúÏã†ÏùºÏàòÎ°ù Ïó¨Îü¨ Î≤ÑÌÇ∑Ïóê ÎèôÏãúÏóê ÏÜçÌï®
                if created_at >= cutoffs_for_windows["7d"]:
                    buckets.append("7d")
                if created_at >= cutoffs_for_windows["1m"]:
                    buckets.append("1m")
                if created_at >= cutoffs_for_windows["3m"]:
                    buckets.append("3m")
                windows_bucket = ";".join(buckets)

            # posts-out: "Í∏∞Í∞Ñ ÌïÑÌÑ∞Î•º ÌÜµÍ≥ºÌïú Question Ï†ÑÏ≤¥"Î•º Ï†ÄÏû• (Í∏∞Ï°¥ ÎèôÏûë Ïú†ÏßÄ)
            # - ÎÑàÍ∞Ä ÏõêÌïòÎ©¥ tech Îß§Ïπ≠Îêú Í∏ÄÎßå Ï†ÄÏû•ÌïòÎèÑÎ°ù ÏïÑÎûò Î∏îÎ°ùÏùÑ seen_in_this_post Ïù¥ÌõÑÎ°ú ÏòÆÍ∏∞Î©¥ Îê®
            if posts_out_opt:
                filtered_posts_rows.append({
                    "post_id": post_id,
                    "created_at": created_at.isoformat() if created_at else "",
                    "url": f"https://stackoverflow.com/questions/{post_id}",
                    "title": normalize_spaces(title).replace("\n", " ").replace("\r", " "),
                    "view_count": view_count,
                    "tags": tags,
                    "windows": windows_bucket,  # üê∂ [ADD]
                    "_created_at_dt": created_at,
                })

            text = normalize_post_text(title, body, tags)
            post_tokens = TOKEN_RE.findall(text)
            post_tok_set = set(post_tokens)

            seen_in_this_post = set()

            # 1) Îã®Ïùº ÌÜ†ÌÅ∞ Í∏∞Ïà†
            for tok in post_tok_set:
                for tech in single_index.get(tok, []):
                    if tech in seen_in_this_post:
                        continue
                    mention_count[tech] += 1
                    total_views[tech] += view_count
                    seen_in_this_post.add(tech)

            # 2) Îã§Ï§ë ÌÜ†ÌÅ∞ Í∏∞Ïà†
            for tok in post_tok_set:
                for tech in multi_index.get(tok, []):
                    if tech in seen_in_this_post:
                        continue
                    tech_tokens = tech_tokens_map[tech]
                    if tokens_match(tech_tokens, post_tokens):
                        mention_count[tech] += 1
                        total_views[tech] += view_count
                        seen_in_this_post.add(tech)

            # Îß§Ïπ≠Îêú techÍ∞Ä ÏóÜÏúºÎ©¥ Ïù¥ÌõÑ ÏòµÏÖò ÏûëÏóÖ Ïä§ÌÇµ
            if not seen_in_this_post:
                if progress and scanned % progress == 0:
                    self.stdout.write(f"scanned={scanned:,}")
                continue

            # 3) ÌäπÏ†ï Í∏∞Ïà†(detail_tech)Ïùò topN Ïú†ÏßÄ (Ï°∞ÌöåÏàò Í∏∞Ï§Ä)
            if detail_tech and detail_tech in seen_in_this_post:
                heapq.heappush(detail_heap, (view_count, post_id, title))
                if len(detail_heap) > topn:
                    heapq.heappop(detail_heap)

            # 4) techÎ≥Ñ topN Ïú†ÏßÄ
            if with_top_posts:
                for tech in seen_in_this_post:
                    h = top_posts_by_tech[tech]
                    heapq.heappush(h, (view_count, post_id, title))
                    if len(h) > topn:
                        heapq.heappop(h)

            # 5) DB Ï†ÄÏû•
            if save_db:
                url = f"https://stackoverflow.com/questions/{post_id}"

                article, created = Article.objects.get_or_create(
                    url=url,
                    defaults={
                        "source": "stackoverflow",
                        "view_count": view_count,
                        "external_created_at": created_at,
                    },
                )

                update_fields = []
                if article.view_count != view_count:
                    article.view_count = view_count
                    update_fields.append("view_count")

                if created_at is not None and article.external_created_at != created_at:
                    article.external_created_at = created_at
                    update_fields.append("external_created_at")

                if update_fields:
                    update_fields.append("updated_at")
                    article.save(update_fields=update_fields)

                created_tech_ids = []
                with transaction.atomic():
                    for tech in seen_in_this_post:
                        ts = db_tech_map.get(tech)
                        if not ts:
                            continue

                        rel, rel_created = ArticleStack.objects.get_or_create(
                            article=article,
                            tech_stack=ts,
                        )
                        if rel_created:
                            created_tech_ids.append(ts.id)

                    if created_tech_ids:
                        TechStack.objects.filter(id__in=created_tech_ids).update(
                            article_stack_count=F("article_stack_count") + 1
                        )

            if progress and scanned % progress == 0:
                self.stdout.write(f"scanned={scanned:,}")

        # ---- posts-out Ï†ÄÏû• ----
        if posts_out_opt:
            if posts_order == "views":
                filtered_posts_rows.sort(key=lambda r: int(r.get("view_count") or 0), reverse=True)
            elif posts_order == "date":
                filtered_posts_rows.sort(
                    key=lambda r: (
                        r.get("_created_at_dt") is not None,
                        r.get("_created_at_dt") or datetime.min.replace(tzinfo=timezone.utc),
                    ),
                    reverse=True,
                )

            for r in filtered_posts_rows:
                r.pop("_created_at_dt", None)

            posts_out_path = Path(posts_out_opt).expanduser()
            posts_out_path.parent.mkdir(parents=True, exist_ok=True)

            with posts_out_path.open("w", newline="", encoding="utf-8") as pf:
                # üê∂ [MOD] windows Ïª¨Îüº Ï∂îÍ∞Ä
                pw = csv.DictWriter(
                    pf,
                    fieldnames=["post_id", "created_at", "url", "title", "view_count", "tags", "windows"],
                )
                pw.writeheader()
                pw.writerows(filtered_posts_rows)

            self.stdout.write(self.style.SUCCESS(f"Filtered posts saved: {posts_out_path}"))

        # ---- Î©îÏù∏ out.csv Ï†ÄÏû• ----
        rows = []
        for tech in techs:
            m = mention_count[tech]
            v = total_views[tech]
            row = {
                "tech": tech,
                "mentions": m,
                "total_views": v,
                "avg_views_per_mention": (v / m) if m else 0,
            }

            if with_top_posts:
                heap = top_posts_by_tech.get(tech, [])
                top_posts = sorted(heap, reverse=True)
                parts = []
                for vc, pid, t in top_posts:
                    url = f"https://stackoverflow.com/questions/{pid}"
                    t_clean = normalize_spaces(t).replace("\n", " ").replace("\r", " ")
                    parts.append(f"{vc}|{url}|{t_clean}")
                row["top_posts"] = " ; ".join(parts)

            rows.append(row)

        rows.sort(key=lambda r: r["total_views"], reverse=True)

        out_path.parent.mkdir(parents=True, exist_ok=True)

        fieldnames = ["tech", "mentions", "total_views", "avg_views_per_mention"]
        if with_top_posts:
            fieldnames.append("top_posts")

        with out_path.open("w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(rows)

        # ---- detail-tech Ï†ÄÏû• ----
        if detail_tech:
            if detail_out_opt:
                detail_out_path = Path(detail_out_opt).expanduser()
            else:
                detail_out_path = out_path.with_name(f"{detail_tech}_top_posts_{topn}.csv")

            detail_out_path.parent.mkdir(parents=True, exist_ok=True)

            detail_rows = []
            for vc, pid, t in sorted(detail_heap, reverse=True):
                detail_rows.append({
                    "tech": detail_tech,
                    "post_id": pid,
                    "url": f"https://stackoverflow.com/questions/{pid}",
                    "view_count": vc,
                    "title": normalize_spaces(t).replace("\n", " ").replace("\r", " "),
                })

            with detail_out_path.open("w", newline="", encoding="utf-8") as df:
                dw = csv.DictWriter(df, fieldnames=["tech", "post_id", "url", "view_count", "title"])
                dw.writeheader()
                dw.writerows(detail_rows)

            self.stdout.write(self.style.SUCCESS(f"Detail saved: {detail_out_path}"))

        self.stdout.write(self.style.SUCCESS(f"Done. scanned={scanned:,} output={out_path}"))
