"""
Posts.xmlì—ì„œ ë‚ ì§œë³„ ê¸°ìˆ  ìŠ¤íƒ ì–¸ê¸‰ëŸ‰ ì§‘ê³„ ë° tech_trend í…Œì´ë¸”ì˜ article_mention_count, article_change_rate ì—…ë°ì´íŠ¸
"""
import re
from collections import defaultdict
from pathlib import Path
from xml.etree.ElementTree import iterparse
from datetime import datetime, date, timedelta, timezone

from django.core.management.base import BaseCommand
from django.utils import timezone as django_timezone
from django.db import transaction

from apps.trends.models import TechStack, TechTrend


# analyze_stackoverflow.pyì™€ ë™ì¼í•œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
TOKEN_RE = re.compile(r"[a-z0-9\+\#\.\-]+")

NOISE_TECHS = {
    "d", "q",
    "make", "simple", "mean", "parse", "render", "echo", "stream", "buffer", "heap",
    "box", "hub", "dash", "flux", "salt", "ent", "tower", "buddy",
    "play", "linear", "segment", "prism", "foundation", "slick", "realm", "crystal",
}
KNOWN_SHORT_TECHS = {"go", "r", "d3", "qt"}


def is_noise_tech(normalized_tech: str) -> bool:
    if normalized_tech in KNOWN_SHORT_TECHS:
        return False
    if not normalized_tech or normalized_tech in NOISE_TECHS:
        return True
    toks = TOKEN_RE.findall(normalized_tech)
    if not toks:
        return True
    if len(toks) == 1:
        t = toks[0]
        if t.isalpha() and len(t) <= 2:
            return True
    return False


def normalize_spaces(s: str) -> str:
    return " ".join((s or "").split())


def normalize_tech_name(name: str) -> str:
    return normalize_spaces(name).lower()


def normalize_tags(tags: str) -> str:
    t = tags or ""
    if "|" in t:
        return normalize_spaces(t.replace("|", " "))
    return normalize_spaces(t.replace("><", "> <").replace("<", " ").replace(">", " "))


def normalize_post_text(title: str, body: str, tags: str) -> str:
    tags_clean = normalize_tags(tags)
    text = f"{title or ''} {body or ''} {tags_clean}"
    return normalize_spaces(text).lower()


def parse_creation_dt(s: str) -> datetime | None:
    """CreationDate -> datetime(UTC) (analyze_stackoverflow.pyì™€ ë™ì¼)"""
    if not s:
        return None
    try:
        # Zê°€ ìˆìœ¼ë©´ +00:00ë¡œ ë°”ê¿”ì„œ íŒŒì‹±
        dt = datetime.fromisoformat(s.replace("Z", "+00:00"))
        # tz ì—†ëŠ” ê°’ì´ë©´ UTCë¡œ ê°•ì œ
        if dt.tzinfo is None:
            return dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except ValueError:
        return None


def iter_posts_with_date(posts_xml_path: Path):
    """Posts.xmlì„ íŒŒì‹±í•˜ì—¬ (post_id, post_type, title, body, tags, created_at) ë°˜í™˜"""
    context = iterparse(posts_xml_path, events=("end",))
    try:
        for _, elem in context:
            if elem.tag != "row":
                continue

            a = elem.attrib
            post_id = a.get("Id") or ""
            post_type = a.get("PostTypeId") or ""
            title = a.get("Title") or ""
            body = a.get("Body") or ""
            tags = a.get("Tags") or ""
            created_raw = a.get("CreationDate") or ""

            created_at = parse_creation_dt(created_raw)

            elem.clear()
            yield post_id, post_type, title, body, tags, created_at
    except Exception as e:
        import sys
        print(f"Warning: XML parsing error: {e}", file=sys.stderr)


def tokens_match(tech_tokens: list[str], post_tokens: list[str]) -> bool:
    """tech í† í° ì‹œí€€ìŠ¤ê°€ post_tokensì— ì—°ì†ìœ¼ë¡œ ë“±ì¥í•˜ëŠ”ì§€ í™•ì¸"""
    if not tech_tokens:
        return False
    L = len(tech_tokens)
    if L > len(post_tokens):
        return False
    for i in range(len(post_tokens) - L + 1):
        if post_tokens[i:i + L] == tech_tokens:
            return True
    return False


def build_tech_index(tech_stacks):
    """TechStack ì¿¼ë¦¬ì…‹ì„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜"""
    single_index = defaultdict(list)
    multi_index = defaultdict(list)
    tech_tokens_map = {}
    tech_id_map = {}  # normalized_tech -> tech_stack_id

    for tech_stack in tech_stacks:
        tech_name = normalize_tech_name(tech_stack.name)
        if is_noise_tech(tech_name):
            continue

        tokens = TOKEN_RE.findall(tech_name)
        if not tokens:
            continue

        tech_tokens_map[tech_name] = tokens
        tech_id_map[tech_name] = tech_stack.id

        if len(tokens) == 1:
            single_index[tokens[0]].append(tech_name)
        else:
            multi_index[tokens[0]].append(tech_name)

    return single_index, multi_index, tech_tokens_map, tech_id_map


class Command(BaseCommand):
    help = 'Posts.xmlì—ì„œ ë‚ ì§œë³„ ê¸°ìˆ  ìŠ¤íƒ ì–¸ê¸‰ëŸ‰ì„ ì§‘ê³„í•˜ì—¬ tech_trend í…Œì´ë¸”ì˜ article_mention_count, article_change_rateë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.'

    def add_arguments(self, parser):
        parser.add_argument(
            '--posts',
            type=str,
            required=True,
            help='Posts.xml íŒŒì¼ ê²½ë¡œ'
        )
        parser.add_argument(
            '--progress',
            type=int,
            default=10000,
            help='ì§„í–‰ ìƒí™© ì¶œë ¥ ê°„ê²© (ê¸°ë³¸ê°’: 10000ê°œ)'
        )
        parser.add_argument(
            '--days',
            type=int,
            default=None,
            help='í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ ìµœê·¼ Nì¼ ë°ì´í„°ë§Œ ì¶”ì¶œ (ê¸°ë³¸ê°’: None, ì „ì²´ ê¸°ê°„ ì²˜ë¦¬)'
        )
        parser.add_argument(
            '--clear-existing',
            action='store_true',
            help='ê¸°ì¡´ article_mention_count, article_change_rate ë°ì´í„°ë¥¼ ëª¨ë‘ ì‚­ì œ (0ìœ¼ë¡œ ì´ˆê¸°í™”)'
        )

    def handle(self, *args, **options):
        posts_path = Path(options['posts']).expanduser()
        progress_interval = options['progress']
        days = options['days']
        clear_existing = options['clear_existing']

        if not posts_path.exists():
            self.stdout.write(self.style.ERROR(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {posts_path}"))
            return

        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
        if clear_existing:
            self.stdout.write("ğŸ—‘ï¸  ê¸°ì¡´ article_mention_count, article_change_rate ë°ì´í„° ì‚­ì œ ì¤‘...")
            deleted_count = TechTrend.objects.filter(
                article_mention_count__gt=0
            ).update(
                article_mention_count=0,
                article_change_rate=0.0
            )
            self.stdout.write(f"âœ… {deleted_count:,}ê°œ ë ˆì½”ë“œì˜ ê²Œì‹œê¸€ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")

        # ë‚ ì§œ ë²”ìœ„ ê³„ì‚° (daysê°€ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ ì „ì²´ ê¸°ê°„ ì²˜ë¦¬)
        start_date = None
        end_date = None
        if days:
            end_date = django_timezone.now().date()
            start_date = end_date - timedelta(days=days - 1)
            self.stdout.write(f"ğŸ“Š Posts.xmlì—ì„œ ë‚ ì§œë³„ ê¸°ìˆ  ìŠ¤íƒ ì–¸ê¸‰ëŸ‰ ì§‘ê³„ ì‹œì‘...")
            self.stdout.write(f"ğŸ“ Posts.xml: {posts_path}")
            self.stdout.write(f"ğŸ“… ê¸°ê°„: {start_date} ~ {end_date} (ìµœê·¼ {days}ì¼)")
        else:
            self.stdout.write(f"ğŸ“Š Posts.xmlì—ì„œ ë‚ ì§œë³„ ê¸°ìˆ  ìŠ¤íƒ ì–¸ê¸‰ëŸ‰ ì§‘ê³„ ì‹œì‘...")
            self.stdout.write(f"ğŸ“ Posts.xml: {posts_path}")
            self.stdout.write(f"ğŸ“… ê¸°ê°„: ì „ì²´ (ë‚ ì§œ í•„í„°ë§ ì—†ìŒ)")

        # 1. TechStack ë¡œë“œ ë° ì¸ë±ìŠ¤ ìƒì„±
        self.stdout.write("ğŸ” ê¸°ìˆ  ìŠ¤íƒ ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        tech_stacks = TechStack.objects.filter(is_deleted=False)
        single_index, multi_index, tech_tokens_map, tech_id_map = build_tech_index(tech_stacks)
        self.stdout.write(f"âœ… {len(tech_id_map)}ê°œ ê¸°ìˆ  ìŠ¤íƒ ë¡œë“œ ì™„ë£Œ")

        # 2. ë‚ ì§œë³„, ê¸°ìˆ  ìŠ¤íƒë³„ ì–¸ê¸‰ëŸ‰ ì§‘ê³„
        trends_data = defaultdict(lambda: defaultdict(int))  # {date: {tech_id: count}}
        scanned = 0
        processed = 0

        self.stdout.write("ğŸ“– Posts.xml íŒŒì‹± ì¤‘...")

        for post_id, post_type, title, body, tags, created_at in iter_posts_with_date(posts_path):
            scanned += 1

            if scanned % progress_interval == 0:
                self.stdout.write(f"  ìŠ¤ìº”: {scanned:,}ê°œ, ì²˜ë¦¬: {processed:,}ê°œ...")

            # Questionë§Œ ì²˜ë¦¬ (PostTypeId == "1")
            if post_type != "1":
                continue

            # ë‚ ì§œ í•„í„°ë§ (ìƒì„±ì¼ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ)
            if not created_at:
                continue

            created_date = created_at.date()
            
            # ë‚ ì§œ í•„í„°ë§ (days ì˜µì…˜ì´ ì§€ì •ëœ ê²½ìš°ë§Œ)
            if days and (created_date < start_date or created_date > end_date):
                continue

            processed += 1

            # ê²Œì‹œê¸€ í…ìŠ¤íŠ¸ ì •ê·œí™” ë° í† í°í™”
            text = normalize_post_text(title, body, tags)
            post_tokens = TOKEN_RE.findall(text)
            post_tok_set = set(post_tokens)

            seen_in_this_post = set()

            # ë‹¨ì¼ í† í° ê¸°ìˆ  ë§¤ì¹­
            for tok in post_tok_set:
                for tech_name in single_index.get(tok, []):
                    if tech_name in seen_in_this_post:
                        continue
                    tech_id = tech_id_map[tech_name]
                    trends_data[created_date][tech_id] += 1
                    seen_in_this_post.add(tech_name)

            # ë‹¤ì¤‘ í† í° ê¸°ìˆ  ë§¤ì¹­
            for tok in post_tok_set:
                for tech_name in multi_index.get(tok, []):
                    if tech_name in seen_in_this_post:
                        continue
                    tech_tokens = tech_tokens_map[tech_name]
                    if tokens_match(tech_tokens, post_tokens):
                        tech_id = tech_id_map[tech_name]
                        trends_data[created_date][tech_id] += 1
                        seen_in_this_post.add(tech_name)

        self.stdout.write(f"âœ… ì´ {scanned:,}ê°œ ê²Œì‹œê¸€ ìŠ¤ìº”, {processed:,}ê°œ Question ì²˜ë¦¬ ì™„ë£Œ")
        
        # ì§‘ê³„ëœ ë°ì´í„° í™•ì¸
        total_mentions = sum(sum(counts.values()) for counts in trends_data.values())
        self.stdout.write(f"ğŸ“Š ì§‘ê³„ ì™„ë£Œ: {len(trends_data)}ì¼, ì´ {total_mentions:,}ê°œ ì–¸ê¸‰")
        if trends_data:
            sample_date = sorted(trends_data.keys())[0]
            sample_count = len(trends_data[sample_date])
            self.stdout.write(f"   ì˜ˆì‹œ: {sample_date} - {sample_count}ê°œ ê¸°ìˆ  ìŠ¤íƒ")

        # 3. tech_trend í…Œì´ë¸”ì— ì €ì¥/ì—…ë°ì´íŠ¸
        # ê° ë‚ ì§œë³„ë¡œ ì „ì²´ ê¸°ìˆ  ìŠ¤íƒ ì–¸ê¸‰ëŸ‰ ëŒ€ë¹„ ê° ê¸°ìˆ  ìŠ¤íƒì˜ ì–¸ê¸‰ëŸ‰ ë¹„ìœ¨(%)ì„ ê³„ì‚°í•˜ì—¬ ì €ì¥
        self.stdout.write("ğŸ’¾ tech_trend í…Œì´ë¸” ì €ì¥ ì¤‘...")
        created_count = 0
        updated_count = 0

        sorted_dates = sorted(trends_data.keys())

        with transaction.atomic():
            for ref_date in sorted_dates:
                tech_counts = trends_data[ref_date]

                # í•´ë‹¹ ë‚ ì§œì˜ ì „ì²´ ê¸°ìˆ  ìŠ¤íƒ ì–¸ê¸‰ëŸ‰ í•©ê³„ ê³„ì‚°
                total_mentions = sum(tech_counts.values())
                
                if total_mentions == 0:
                    # ì–¸ê¸‰ëŸ‰ì´ ì—†ìœ¼ë©´ ë¹„ìœ¨ ê³„ì‚° ë¶ˆê°€, ëª¨ë“  ê¸°ìˆ  ìŠ¤íƒì— 0.0 ì €ì¥
                    for tech_id in tech_counts.keys():
                        try:
                            tech_stack = TechStack.objects.get(id=tech_id, is_deleted=False)
                        except TechStack.DoesNotExist:
                            continue
                        
                        trend, created = TechTrend.objects.update_or_create(
                            tech_stack=tech_stack,
                            reference_date=ref_date,
                            defaults={
                                'article_mention_count': 0,
                                'article_change_rate': 0.0,
                                'is_deleted': False,
                            }
                        )
                        if created:
                            created_count += 1
                        else:
                            updated_count += 1
                    continue

                # ê° ê¸°ìˆ  ìŠ¤íƒë³„ë¡œ ë¹„ìœ¨ ê³„ì‚° ë° ì €ì¥
                for tech_id, mention_count in tech_counts.items():
                    try:
                        tech_stack = TechStack.objects.get(id=tech_id, is_deleted=False)
                    except TechStack.DoesNotExist:
                        continue

                    # ì „ì²´ ëŒ€ë¹„ ë¹„ìœ¨ ê³„ì‚° (%)
                    article_change_rate = (mention_count / total_mentions) * 100

                    # TechTrend ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸ (article í•„ë“œë§Œ ì—…ë°ì´íŠ¸, job í•„ë“œëŠ” ìœ ì§€)
                    trend, created = TechTrend.objects.update_or_create(
                        tech_stack=tech_stack,
                        reference_date=ref_date,
                        defaults={
                            'article_mention_count': mention_count,
                            'article_change_rate': round(article_change_rate, 2),
                            'is_deleted': False,
                        }
                    )

                    if created:
                        created_count += 1
                    else:
                        updated_count += 1

        self.stdout.write(
            self.style.SUCCESS(
                f"âœ… ì™„ë£Œ! ìƒì„±: {created_count:,}ê°œ, ì—…ë°ì´íŠ¸: {updated_count:,}ê°œ"
            )
        )
